import numpy as np
from scipy.io import loadmat
from scipy.optimize import minimize
from RandInitialize import initialise
from Model import neural_network
from Prediction import predict

# Loading mat file
data = loadmat('mnist-original.mat')

# Extracting features from mat file and adding bias unit to the first layer
X = data['data'].transpose() / 255
m = X.shape[0]
one_matrix = np.ones((m, 1))
X = np.append(one_matrix, X, axis=1)

# Extracting labels from mat file
y = data['label'].flatten()

# Splitting data into training set with 60,000 examples
X_train, y_train = X[:60000, :], y[:60000]

# Splitting data into testing set with 10,000 examples
X_test, y_test = X[60000:, :], y[60000:]

# Number of examples and input size
input_layer_size = X.shape[1]

# Hidden layer size and number of output labels
hidden_layer_size, num_labels = 100, 10

# Randomly initialising Thetas
initial_Theta1 = initialise(hidden_layer_size, input_layer_size)
initial_Theta2 = initialise(num_labels, hidden_layer_size)

# Unrolling parameters into a single column vector
initial_nn_params = np.concatenate((initial_Theta1.flatten(), initial_Theta2.flatten()))

# Maximum number of iterations for optimization
maxiter = 100

# Regularization parameter to avoid overfitting
lambda_reg = 0.1

# Arguments to be passed to the neural network function
myargs = (input_layer_size, hidden_layer_size, num_labels, X_train, y_train, lambda_reg)

# Calling minimize function to minimize cost function and train weights
results = minimize(neural_network, x0=initial_nn_params, args=myargs,
                   options={'disp': True, 'maxiter': maxiter}, method="L-BFGS-B", jac=True)

nn_params = results["x"]

# Trained Theta is extracted
# Weights are split back to Theta1, Theta2
Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1))
Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1))

# Activation for the second layer
z2 = np.dot(X, Theta1.transpose())
a2 = 1 / (1 + np.exp(-z2))
# Adding bias unit to the hidden layer
a2 = np.append(one_matrix, a2, axis=1)

# Activation for the third layer
z3 = np.dot(a2, Theta2.transpose())
a3 = 1 / (1 + np.exp(-z3))

# Checking test set accuracy of our model
pred = predict(Theta1, Theta2, X_test)
print('Test Set Accuracy: {:f}'.format((np.mean(pred == y_test) * 100)))

# Checking train set accuracy of our model
pred = predict(Theta1, Theta2, X_train)
print('Training Set Accuracy: {:f}'.format((np.mean(pred == y_train) * 100)))

# Evaluating precision of our model
true_positive = np.sum(pred == y_train)
false_positive = len(y_train) - true_positive
print('Precision =', true_positive / (true_positive + false_positive))

# Saving Thetas in .txt file
np.savetxt('Theta1.txt', Theta1, delimiter=' ')
np.savetxt('Theta2.txt', Theta2, delimiter=' ')
